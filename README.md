# PRODIGY_GA_01 – GPT-2 Fine-Tuned Text Generation

## Overview
This project demonstrates fine-tuning a GPT-2 language model on a custom dataset for text generation using Hugging Face Transformers.

## Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- Datasets
- Git & GitHub

## Project Structure
- train.py – Fine-tunes GPT-2 on custom text data
- generate.py – Generates text using the trained model
- data.txt – Training dataset
- requirements.txt – Dependencies

## How to Run
1. Install dependencies:
   pip install -r requirements.txt
2. Train the model:
   python train.py
3. Generate text:
   python generate.py

Learning Outcome
>Understanding transformer-based laguage models
>Fine
>tuning GPT-2 on custom data
>Managing ML projects with github
