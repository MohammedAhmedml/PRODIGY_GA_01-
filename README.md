# PRODIGY_GA_01 – GPT-2 Fine-Tuned Text Generation

## Overview
This project demonstrates fine-tuning a GPT-2 language model on a custom dataset for text generation using Hugging Face Transformers.

## Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- Datasets
- Git & GitHub

## Project Structure
- train.py – Fine-tunes GPT-2 on custom text data
- generate.py – Generates text using the trained model
- data.txt – Training dataset
- requirements.txt – Dependencies

## How to Run
1. Install dependencies:
   ```bash
   pip install -r requirements.txt
